behaviors:
  TerrainWalker:
    trainer_type: ppo
    hyperparameters:
      batch_size: 1024
      buffer_size: 20480
      learning_rate: 0.0003
      beta: 0.005
      epsilon: 0.2
      lambd: 0.95
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 256
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.995
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 30000000
    time_horizon: 1000
    summary_freq: 100000
torch_settings:
  device: cuda
  # Walker:
  #   trainer_type: ppo
  #   hyperparameters:
  #     batch_size: 2048
  #     buffer_size: 20480
  #     learning_rate: 0.0003
  #     beta: 0.005
  #     epsilon: 0.2
  #     lambd: 0.95
  #     num_epoch: 3
  #     learning_rate_schedule: linear
  #   network_settings:
  #     normalize: true
  #     hidden_units: 256
  #     num_layers: 3
  #     vis_encode_type: simple
  #   reward_signals:
  #     extrinsic:
  #       gamma: 0.995
  #       strength: 1.0
  #     gail:
  #       gamma: 0.99
  #       strength: 1.0
  #       network_settings:
  #         normalize: true
  #         hidden_units: 128
  #         num_layers: 2
  #         vis_encode_type: simple
  #       learning_rate: 0.0003
  #       use_actions: false
  #       use_vail: false
  #       demo_path: Assets/ML-Agents/Examples/Walker/Demos/ExpertWalker.demo
  #   keep_checkpoints: 5
  #   max_steps: 30000000
  #   time_horizon: 1000
  #   summary_freq: 30000
  #   behavioral_cloning:
  #     demo_path: Assets/ML-Agents/Examples/Walker/Demos/ExpertWalker.demo
  #     steps: 50000
  #     strength: 0.5
  #     samples_per_update: 0
  # mywalker:
  #   trainer_type: ppo
  #   hyperparameters:
  #     batch_size: 128
  #     buffer_size: 2048
  #     learning_rate: 0.0003
  #     beta: 0.01
  #     epsilon: 0.2
  #     lambd: 0.95
  #     num_epoch: 3
  #     learning_rate_schedule: linear
  #   network_settings:
  #     normalize: false
  #     hidden_units: 256
  #     num_layers: 2
  #     vis_encode_type: simple
  #   reward_signals:
  #     extrinsic:
  #       gamma: 0.99
  #       strength: 1.0
  #     # gail:
  #     #   gamma: 0.99
  #     #   strength: 0.01
  #     #   network_settings:
  #     #     normalize: false
  #     #     hidden_units: 128
  #     #     num_layers: 2
  #     #     vis_encode_type: simple
  #     #   learning_rate: 0.0003
  #     #   use_actions: false
  #     #   use_vail: false
  #     #   demo_path: Project/Assets/ML-Agents/Examples/PushBlock/Demos/ExpertPushBlock.demo
  #   keep_checkpoints: 5
  #   max_steps: 1000000
  #   time_horizon: 64
  #   summary_freq: 50000
  #   # behavioral_cloning:
  #   #   demo_path: Project/Assets/ML-Agents/Examples/PushBlock/Demos/ExpertPushBlock.demo
  #   #   steps: 50000
  #   #   strength: 1.0
  #   #   samples_per_update: 0
  # Sample1MoveToGoal:
  #   trainer_type: ppo
  #   hyperparameters:
  #     batch_size: 10
  #     buffer_size: 100
  #     learning_rate: 0.0003
  #     beta: 0.0005
  #     epsilon: 0.2
  #     lambd: 0.99
  #     num_epoch: 3
  #     shared_critic: false
  #     learning_rate_schedule: linear
  #     beta_schedule: constant
  #     epsilon_schedule: linear
  #   checkpoint_interval: 500000
  #   network_settings:
  #     normalize: false
  #     hidden_units: 128
  #     num_layers: 2
  #   reward_signals:
  #     extrinsic:
  #       gamma: 0.99
  #       strength: 1.0
  #       network_settings:
  #         hidden_units: 128
  #         num_layers: 2
  #   keep_checkpoints: 5
  #   even_checkpoints: false
  #   max_steps: 500000
  #   time_horizon: 64
  #   summary_freq: 10000
